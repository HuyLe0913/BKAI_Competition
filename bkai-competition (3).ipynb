{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":30892,"databundleVersionId":2715462,"sourceType":"competition"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchgeometry\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport cv2\nimport pandas as pd\nimport torch\nfrom torchvision.transforms import functional as T\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torchvision.transforms import Resize, PILToTensor, ToPILImage, Compose, InterpolationMode\nfrom torchgeometry.losses import one_hot\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torchvision.transforms as transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T08:56:37.248875Z","iopub.execute_input":"2024-11-23T08:56:37.249361Z","iopub.status.idle":"2024-11-23T08:56:46.842255Z","shell.execute_reply.started":"2024-11-23T08:56:37.249294Z","shell.execute_reply":"2024-11-23T08:56:46.841223Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchgeometry in /opt/conda/lib/python3.10/site-packages (0.1.2)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from torchgeometry) (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->torchgeometry) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->torchgeometry) (1.3.0)\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"torch.backends.cudnn.benchmark = True\n!pip install wandb\nimport wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T08:56:46.844975Z","iopub.execute_input":"2024-11-23T08:56:46.845791Z","iopub.status.idle":"2024-11-23T08:56:56.459169Z","shell.execute_reply.started":"2024-11-23T08:56:46.845745Z","shell.execute_reply":"2024-11-23T08:56:56.458109Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.18.3)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.15.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (70.0.0)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"# Check compute device\ndevice = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)\nprint(f\"Using {device} device\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T08:56:56.460781Z","iopub.execute_input":"2024-11-23T08:56:56.461120Z","iopub.status.idle":"2024-11-23T08:56:56.468220Z","shell.execute_reply.started":"2024-11-23T08:56:56.461086Z","shell.execute_reply":"2024-11-23T08:56:56.467137Z"}},"outputs":[{"name":"stdout","text":"Using cuda device\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"epochs = 100\ntrain_split = 0.8","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T08:56:56.469435Z","iopub.execute_input":"2024-11-23T08:56:56.469695Z","iopub.status.idle":"2024-11-23T08:56:56.482305Z","shell.execute_reply.started":"2024-11-23T08:56:56.469669Z","shell.execute_reply":"2024-11-23T08:56:56.481303Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"class RandomGamma:\n    def __init__(self, gamma_range=(0.7, 1.3), p=0.2):\n        self.gamma_range = gamma_range\n        self.p = p\n\n    def __call__(self, img):\n        if torch.rand(1).item() < self.p:\n            gamma = torch.empty(1).uniform_(*self.gamma_range).item()\n            return T.adjust_gamma(img, gamma, gain=1)\n        return img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T08:56:56.485708Z","iopub.execute_input":"2024-11-23T08:56:56.486092Z","iopub.status.idle":"2024-11-23T08:56:56.495727Z","shell.execute_reply.started":"2024-11-23T08:56:56.486050Z","shell.execute_reply":"2024-11-23T08:56:56.494711Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, images_path, masks_path, transform):\n        super(CustomDataset, self).__init__()\n        images_list = os.listdir(images_path)\n        masks_list = os.listdir(masks_path)\n        \n        images_list = [images_path + image_name for image_name in images_list]\n        self.images_list = images_list\n        masks_list = [masks_path + mask_name for mask_name in masks_list]\n        self.masks_list = masks_list\n        self.transform = transform\n\n    def __getitem__(self, index):\n        img_path = self.images_list[index]\n        mask_path = self.masks_list[index]\n        \n        # Open image and mask\n        data = Image.open(img_path)\n        label = Image.open(mask_path)\n\n        # Apply transformations\n        data = self.transform(data)\n        label = self.transform(label)\n        \n        # Normalize the data (if not already done in the transform)\n        data = data / 255.0  # Normalize image to [0, 1] range if transform doesn't handle it\n        \n        # Threshold label to binary mask (or multi-class if needed)\n        label = torch.where(label > 0.65, 1.0, 0.0)  # Apply thresholding\n        \n        # Set the third channel to a small value if you need to manipulate it specifically\n        if label.shape[0] > 2:  # Check if the label has more than 2 channels\n            label[2, :, :] = 0.0001\n            \n        # Convert the label to class indices (if label is one-hot encoded)\n        label = torch.argmax(label, dim=0).type(torch.int64)  # Get class indices\n\n        return data, label\n\n    def __len__(self):\n        return len(self.images_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T08:56:56.496964Z","iopub.execute_input":"2024-11-23T08:56:56.497252Z","iopub.status.idle":"2024-11-23T08:56:56.507242Z","shell.execute_reply.started":"2024-11-23T08:56:56.497224Z","shell.execute_reply":"2024-11-23T08:56:56.506442Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"transforms = transforms.Compose([transforms.Resize((224, 224)), \n                                transforms.RandomHorizontalFlip(p=0.5),\n                                transforms.RandomVerticalFlip(p=0.5),\n                                RandomGamma(gamma_range=(0.7, 1.3), p=0.2),\n                                transforms.ToTensor()])\ndataset = CustomDataset('/kaggle/input/bkai-igh-neopolyp/train/train/', '/kaggle/input/bkai-igh-neopolyp/train_gt/train_gt/', transforms)\ntrain_dataset, val_dataset = random_split(dataset, \n                                    [int(train_split * len(dataset)) , \n                                     len(dataset) - int(train_split * len(dataset))])\nval_loader = DataLoader(val_dataset, batch_size=4, shuffle=True)\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T08:56:56.508624Z","iopub.execute_input":"2024-11-23T08:56:56.508969Z","iopub.status.idle":"2024-11-23T08:56:56.528505Z","shell.execute_reply.started":"2024-11-23T08:56:56.508929Z","shell.execute_reply":"2024-11-23T08:56:56.527446Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"!pip install segmentation-models-pytorch\nimport segmentation_models_pytorch as smp\n\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"efficientnet-b7\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=3\n)\nmodel.to(device)\n#print(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T08:56:56.529729Z","iopub.execute_input":"2024-11-23T08:56:56.530065Z","iopub.status.idle":"2024-11-23T08:57:07.027370Z","shell.execute_reply.started":"2024-11-23T08:56:56.530018Z","shell.execute_reply":"2024-11-23T08:57:07.025991Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: segmentation-models-pytorch in /opt/conda/lib/python3.10/site-packages (0.3.4)\nRequirement already satisfied: efficientnet-pytorch==0.7.1 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.7.1)\nRequirement already satisfied: huggingface-hub>=0.24.6 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.25.1)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (10.3.0)\nRequirement already satisfied: pretrainedmodels==0.7.4 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.7.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (1.16.0)\nRequirement already satisfied: timm==0.9.7 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.9.7)\nRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.19.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.4.0)\nRequirement already satisfied: munch in /opt/conda/lib/python3.10/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.0.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.7->segmentation-models-pytorch) (6.0.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.7->segmentation-models-pytorch) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (4.12.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.26.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n","output_type":"stream"},{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"UnetPlusPlus(\n  (encoder): EfficientNetEncoder(\n    (_conv_stem): Conv2dStaticSamePadding(\n      3, 64, kernel_size=(3, 3), stride=(2, 2), bias=False\n      (static_padding): ZeroPad2d((0, 1, 0, 1))\n    )\n    (_bn0): BatchNorm2d(64, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n    (_blocks): ModuleList(\n      (0): MBConvBlock(\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          64, 64, kernel_size=(3, 3), stride=[1, 1], groups=64, bias=False\n          (static_padding): ZeroPad2d((1, 1, 1, 1))\n        )\n        (_bn1): BatchNorm2d(64, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          64, 16, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          16, 64, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (1-3): 3 x MBConvBlock(\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False\n          (static_padding): ZeroPad2d((1, 1, 1, 1))\n        )\n        (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          32, 8, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          8, 32, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (4): MBConvBlock(\n        (_expand_conv): Conv2dStaticSamePadding(\n          32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          192, 192, kernel_size=(3, 3), stride=[2, 2], groups=192, bias=False\n          (static_padding): ZeroPad2d((0, 1, 0, 1))\n        )\n        (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          192, 8, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          8, 192, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (5-10): 6 x MBConvBlock(\n        (_expand_conv): Conv2dStaticSamePadding(\n          48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False\n          (static_padding): ZeroPad2d((1, 1, 1, 1))\n        )\n        (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          288, 12, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          12, 288, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (11): MBConvBlock(\n        (_expand_conv): Conv2dStaticSamePadding(\n          48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          288, 288, kernel_size=(5, 5), stride=[2, 2], groups=288, bias=False\n          (static_padding): ZeroPad2d((1, 2, 1, 2))\n        )\n        (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          288, 12, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          12, 288, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          288, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (12-17): 6 x MBConvBlock(\n        (_expand_conv): Conv2dStaticSamePadding(\n          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False\n          (static_padding): ZeroPad2d((2, 2, 2, 2))\n        )\n        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          480, 20, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          20, 480, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (18): MBConvBlock(\n        (_expand_conv): Conv2dStaticSamePadding(\n          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          480, 480, kernel_size=(3, 3), stride=[2, 2], groups=480, bias=False\n          (static_padding): ZeroPad2d((1, 1, 1, 1))\n        )\n        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          480, 20, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          20, 480, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          480, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (19-27): 9 x MBConvBlock(\n        (_expand_conv): Conv2dStaticSamePadding(\n          160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False\n          (static_padding): ZeroPad2d((1, 1, 1, 1))\n        )\n        (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          960, 40, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          40, 960, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (28): MBConvBlock(\n        (_expand_conv): Conv2dStaticSamePadding(\n          160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          960, 960, kernel_size=(5, 5), stride=[1, 1], groups=960, bias=False\n          (static_padding): ZeroPad2d((2, 2, 2, 2))\n        )\n        (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          960, 40, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          40, 960, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          960, 224, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (29-37): 9 x MBConvBlock(\n        (_expand_conv): Conv2dStaticSamePadding(\n          224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False\n          (static_padding): ZeroPad2d((2, 2, 2, 2))\n        )\n        (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          1344, 56, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          56, 1344, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (38): MBConvBlock(\n        (_expand_conv): Conv2dStaticSamePadding(\n          224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          1344, 1344, kernel_size=(5, 5), stride=[2, 2], groups=1344, bias=False\n          (static_padding): ZeroPad2d((1, 2, 1, 2))\n        )\n        (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          1344, 56, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          56, 1344, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          1344, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (39-50): 12 x MBConvBlock(\n        (_expand_conv): Conv2dStaticSamePadding(\n          384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False\n          (static_padding): ZeroPad2d((2, 2, 2, 2))\n        )\n        (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          2304, 96, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          96, 2304, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (51): MBConvBlock(\n        (_expand_conv): Conv2dStaticSamePadding(\n          384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          2304, 2304, kernel_size=(3, 3), stride=[1, 1], groups=2304, bias=False\n          (static_padding): ZeroPad2d((1, 1, 1, 1))\n        )\n        (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          2304, 96, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          96, 2304, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          2304, 640, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(640, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (52-54): 3 x MBConvBlock(\n        (_expand_conv): Conv2dStaticSamePadding(\n          640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn0): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          3840, 3840, kernel_size=(3, 3), stride=(1, 1), groups=3840, bias=False\n          (static_padding): ZeroPad2d((1, 1, 1, 1))\n        )\n        (_bn1): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          3840, 160, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          160, 3840, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(640, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n    )\n    (_conv_head): Conv2dStaticSamePadding(\n      640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False\n      (static_padding): Identity()\n    )\n    (_bn1): BatchNorm2d(2560, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n    (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n    (_dropout): Dropout(p=0.5, inplace=False)\n    (_swish): MemoryEfficientSwish()\n  )\n  (decoder): UnetPlusPlusDecoder(\n    (center): Identity()\n    (blocks): ModuleDict(\n      (x_0_0): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(864, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n      (x_0_1): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(416, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n      (x_1_1): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(304, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n      (x_0_2): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(272, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n      (x_1_2): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(176, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n      (x_2_2): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(128, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n      (x_0_3): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(320, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n      (x_1_3): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(240, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n      (x_2_3): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(176, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n      (x_3_3): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(112, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n      (x_0_4): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n    )\n  )\n  (segmentation_head): SegmentationHead(\n    (0): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): Identity()\n    (2): Activation(\n      (activation): Identity()\n    )\n  )\n)"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"class DiceLoss(nn.Module):\n    def __init__(self, weights):\n        super(DiceLoss, self).__init__()\n        self.eps: float = 1e-6\n        self.weights: torch.Tensor = weights\n    def forward(self, input: torch.Tensor, target: torch.Tensor):\n        # cross entropy loss\n        celoss = nn.CrossEntropyLoss(self.weights)(input, target)\n        \n        # compute softmax over the classes axis\n        input_soft = F.softmax(input, dim=1)\n\n        # create the labels one hot tensor\n        target_one_hot = one_hot(target, num_classes=input.shape[1],\n                                 device=input.device, dtype=input.dtype)\n\n        # compute the actual dice score\n        dims = (2, 3)\n        intersection = torch.sum(input_soft * target_one_hot, dims)\n        cardinality = torch.sum(input_soft + target_one_hot, dims)\n\n        dice_score = 2. * intersection / (cardinality + self.eps)\n        \n        dice_score = torch.sum(dice_score * self.weights, dim=1)\n        \n        return torch.mean(1. - dice_score) + celoss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T08:57:07.028854Z","iopub.execute_input":"2024-11-23T08:57:07.029177Z","iopub.status.idle":"2024-11-23T08:57:07.037353Z","shell.execute_reply.started":"2024-11-23T08:57:07.029142Z","shell.execute_reply":"2024-11-23T08:57:07.036164Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"weights = torch.Tensor([[0.4, 0.55, 0.05]]).cuda()\ncriterion = DiceLoss(weights)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ntrain_loss_array = []\ntest_loss_array = []\nbest_val_loss = 9999999\nwandb.login(\n    key = \"a999625da52ea7e053c244463d7cee7050b12839\",\n)\nwandb.init(\n    project = \"BKAI_graph\"\n)\n# Training loop\nfor epoch in range(epochs):\n    model.train()\n    train_loss = 0.0\n    for images, masks in train_loader:  # images, masks are (B, C, H, W)\n        images, masks = images.to(device), masks.to(device)\n        outputs = model(images)\n        \n        loss = criterion(outputs, masks.long())  # Use updated DiceLoss\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n    #\n    model.eval()\n    test_loss = 0.0\n    correct = 0\n    total_samples = 0\n    with torch.no_grad():\n        for i, (data, targets) in enumerate(val_loader):\n            data, targets = data.to(device), targets.to(device)\n            outputs = model(data)\n            _, pred = torch.max(outputs, 1)\n            \n            loss = criterion(outputs, targets.long())\n            test_loss += loss.item()\n    if test_loss < best_val_loss:\n        best_val_loss = test_loss\n        checkpoint = { \n            'epoch': epoch,\n            'model': model.state_dict(),\n            'optimizer': optimizer.state_dict(),\n            'train_loss':train_loss,\n            'val_loss': val_loss,\n        }\n        save_path = f'model.pth'\n        torch.save(checkpoint, save_path)\n    train_loss_array.append(train_loss/len(train_loader))\n    test_loss_array.append(test_loss/len(val_loader))\n\n    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Validation Loss: {test_loss/len(val_loader):.4f}\")\n    wandb.log({\n        \"Epoch\": epoch + 1,\n        \"Train Loss\": train_loss / len(train_loader),\n        \"Validation Loss\": test_loss / len(val_loader),\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T08:57:07.038606Z","iopub.execute_input":"2024-11-23T08:57:07.039052Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:oklcvy1h) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.024 MB of 0.024 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a73007e90e9e40cca1d38d5eeb219d4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>Train Loss</td><td>█▄▄▄▃▃▁▁▁▁</td></tr><tr><td>Validation Loss</td><td>█▇▆▃▃▃▂▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Loss</td><td>1.37456</td></tr><tr><td>Validation Loss</td><td>1.33847</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">daily-universe-8</strong> at: <a href='https://wandb.ai/huy01234055137-hanoi-university-of-science-and-technology/BKAI_graph/runs/oklcvy1h' target=\"_blank\">https://wandb.ai/huy01234055137-hanoi-university-of-science-and-technology/BKAI_graph/runs/oklcvy1h</a><br/> View project at: <a href='https://wandb.ai/huy01234055137-hanoi-university-of-science-and-technology/BKAI_graph' target=\"_blank\">https://wandb.ai/huy01234055137-hanoi-university-of-science-and-technology/BKAI_graph</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20241123_084143-oklcvy1h/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:oklcvy1h). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241123_085707-bx0fj3dh</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/huy01234055137-hanoi-university-of-science-and-technology/BKAI_graph/runs/bx0fj3dh' target=\"_blank\">efficient-wood-10</a></strong> to <a href='https://wandb.ai/huy01234055137-hanoi-university-of-science-and-technology/BKAI_graph' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/huy01234055137-hanoi-university-of-science-and-technology/BKAI_graph' target=\"_blank\">https://wandb.ai/huy01234055137-hanoi-university-of-science-and-technology/BKAI_graph</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/huy01234055137-hanoi-university-of-science-and-technology/BKAI_graph/runs/bx0fj3dh' target=\"_blank\">https://wandb.ai/huy01234055137-hanoi-university-of-science-and-technology/BKAI_graph/runs/bx0fj3dh</a>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"for i, (data, label) in enumerate(val_loader):\n     img = data\n     mask = label\n     break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, arr = plt.subplots(4, 3, figsize=(16, 12))\narr[0][0].set_title('Image')\narr[0][1].set_title('Segmentation')\narr[0][2].set_title('Predict')\n\nmodel.eval()\nwith torch.no_grad():\n     predict = model(img.to(device))\n\nfor i in range(4):\n\n     arr[i][0].imshow((img*255)[i].cpu().numpy().transpose(1, 2, 0));\n    \n     arr[i][1].imshow(F.one_hot(mask[i]).float())\n    \n     arr[i][2].imshow(F.one_hot(torch.argmax(predict[i], dim = 0).cpu()).float())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Submission**","metadata":{}},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, images_path, transform):\n        super(TestDataset, self).__init__()\n        \n        images_list = os.listdir(images_path)\n        images_list = [images_path+i for i in images_list]\n        \n        self.images_list = images_list\n        self.transform = transform\n        \n    def __getitem__(self, index):\n        img_path = self.images_list[index]\n        data = Image.open(img_path)\n        h = data.size[1]\n        w = data.size[0]\n        data = self.transform(data) / 255        \n        return data, img_path, h, w\n    \n    def __len__(self):\n        return len(self.images_list)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"path = '/kaggle/input/bkai-igh-neopolyp/test/test/'\ntest_dataset = TestDataset(path, transform)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i, (data, path, h, w) in enumerate(test_loader):\n    img = data\n    break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, arr = plt.subplots(4, 2, figsize=(16, 12))\narr[0][0].set_title('Image');\narr[0][1].set_title('Predict');\n\nmodel.eval()\nwith torch.no_grad():\n    predict = model(img.to(device))\n\nfor i in range(4):\n    arr[i][0].imshow((img*255)[i].cpu().numpy().transpose(1, 2, 0));\n    arr[i][1].imshow(F.one_hot(torch.argmax(predict[i], 0).cpu()).float())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\nif not os.path.isdir(\"/kaggle/working/predicted_masks\"):\n    os.mkdir(\"/kaggle/working/predicted_masks\")\nfor _, (img, path, H, W) in enumerate(test_loader):\n    \n    with torch.no_grad():\n        predicted_mask = model(img.to(device))\n    for i in range(len(a)):\n        image_id = path[i].split('/')[-1].split('.')[0]\n        filename = image_id + \".png\"\n        mask2img = Resize((H[i].item(), W[i].item()), interpolation=InterpolationMode.NEAREST)(ToPILImage()(F.one_hot(torch.argmax(predicted_mask[i], 0)).permute(2, 0, 1).float()))\n        mask2img.save(os.path.join(\"/kaggle/working/predicted_masks/\", filename))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def rle_to_string(runs):\n    return ' '.join(str(x) for x in runs)\n\ndef rle_encode_one_mask(mask):\n    pixels = mask.flatten()\n    pixels[pixels > 0] = 255\n    use_padding = False\n    if pixels[0] or pixels[-1]:\n        use_padding = True\n        pixel_padded = np.zeros([len(pixels) + 2], dtype=pixels.dtype)\n        pixel_padded[1:-1] = pixels\n        pixels = pixel_padded\n    \n    rle = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    if use_padding:\n        rle = rle - 1\n    rle[1::2] = rle[1::2] - rle[:-1:2]\n    return rle_to_string(rle)\n\ndef mask2string(dir):\n    strings = []\n    ids = []\n    ws, hs = [[] for i in range(2)]\n    for image_id in os.listdir(dir):\n        id = image_id.split('.')[0]\n        path = os.path.join(dir, image_id)\n        print(path)\n        img = cv2.imread(path)[:,:,::-1]\n        h, w = img.shape[0], img.shape[1]\n        for channel in range(2):\n            ws.append(w)\n            hs.append(h)\n            ids.append(f'{id}_{channel}')\n            string = rle_encode_one_mask(img[:,:,channel])\n            strings.append(string)\n    r = {\n        'ids': ids,\n        'strings': strings,\n    }\n    return r\n\n\nMASK_DIR_PATH = '/kaggle/working/predicted_masks' # change this to the path to your output mask folder\ndir = MASK_DIR_PATH\nres = mask2string(dir)\ndf = pd.DataFrame(columns=['Id', 'Expected'])\ndf['Id'] = res['ids']\ndf['Expected'] = res['strings']\ndf.to_csv(r'output.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}